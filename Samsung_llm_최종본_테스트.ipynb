{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 코랩 ipynb 파일 실행 매뉴얼\n",
        "1. 먼저 pip install이 모아져 있는 셀을 실행합니다.\n",
        "2. 실행 완료되면 '런타임-세션 다시시작'을 클릭하고 아래 셀을 실행합니다.\n"
      ],
      "metadata": {
        "id": "_LqEpwjqbf0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uvJPu8M3mrv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672afa38-c1a1-48aa-e457-cf3af1f61372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.36.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.25.0.dev0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: Faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.346)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.10)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.69)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.36.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (10.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n",
            "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.10/dist-packages (0.11.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.11.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.9.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.3)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2023.6.15)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.23.5)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.5.2)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.14.1)\n",
            "Requirement already satisfied: python-docx>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.15.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.2.1)\n",
            "Requirement already satisfied: msg-parser in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.5.3)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (20221105)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.5.1)\n",
            "Requirement already satisfied: unstructured-inference==0.7.15 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.7.15)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.3)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.12)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.2)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (8.8.0)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.3.12)\n",
            "Requirement already satisfied: python-pptx<=0.6.23 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.23)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.15->unstructured[all-docs]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.15->unstructured[all-docs]) (0.0.6)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.15->unstructured[all-docs]) (0.19.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.15->unstructured[all-docs]) (4.8.0.76)\n",
            "Requirement already satisfied: onnxruntime<1.16 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.15->unstructured[all-docs]) (1.15.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.15->unstructured[all-docs]) (4.36.0.dev0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]) (10.1.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]) (3.1.9)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]) (23.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.10/dist-packages (from msg-parser->unstructured[all-docs]) (0.47)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (4.66.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (41.0.7)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2023.11.17)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.15->unstructured[all-docs]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.15->unstructured[all-docs]) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.15->unstructured[all-docs]) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.15->unstructured[all-docs]) (3.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.15->unstructured[all-docs]) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.15->unstructured[all-docs]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.15->unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.15->unstructured[all-docs]) (2023.6.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (1.11.4)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (0.10.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (0.16.0+cu118)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (0.3.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.21)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.15->unstructured[all-docs]) (10.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (0.9.12)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (2.0.7)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (2.1.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (4.24.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.15->unstructured[all-docs]) (1.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.15->unstructured[all-docs]) (3.1.1)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.10/dist-packages (8.11.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.346)\n",
            "Requirement already satisfied: elastic-transport<9,>=8 in /usr/local/lib/python3.10/dist-packages (from elasticsearch) (8.10.0)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.10)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.69)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2.0.7)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.10/dist-packages (8.11.0)\n",
            "Requirement already satisfied: elastic-transport<9,>=8 in /usr/local/lib/python3.10/dist-packages (from elasticsearch) (8.10.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2.0.7)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install peft\n",
        "!pip install Faiss-cpu\n",
        "!pip install langchain\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install konlpy\n",
        "!pip install rank_bm25\n",
        "!pip install sentencepiece\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install \"unstructured[all-docs]\"\n",
        "!pip install elasticsearch openai tiktoken langchain\n",
        "!pip install elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "itwgZ7TamoLW"
      },
      "outputs": [],
      "source": [
        "# from langchain.schema import Document\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# from langchain.document_loaders.csv_loader import CSVLoader\n",
        "# from langchain.text_splitter import CharacterTextSplitter\n",
        "# from langchain.vectorstores import FAISS\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.document_loaders import TextLoader\n",
        "# from langchain.document_loaders import DirectoryLoader\n",
        "# from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# import transformers\n",
        "# from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from transformers import TextStreamer, GenerationConfig\n",
        "\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "# from langchain.vectorstores import Chroma\n",
        "# import torch\n",
        "# import re\n",
        "# import base64\n",
        "# import os\n",
        "# import unicodedata\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.image as img\n",
        "# from google.colab import drive\n",
        "\n",
        "# # 도커로 경로 바꾸기\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # csv load\n",
        "# loader = DirectoryLoader(\"/content/drive/MyDrive/[FINAL] 텍스트 전처리 통합\", loader_cls=TextLoader)\n",
        "# data=loader.load()\n",
        "\n",
        "# loader_graph = DirectoryLoader(\"/content/drive/MyDrive/[FINAL] 그래프 전처리\", loader_cls=TextLoader)\n",
        "# data_graph=loader_graph.load()\n",
        "\n",
        "# # text split\n",
        "# # chunk_overlap - 겹쳐서 나오는 내용 조절\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=30)\n",
        "# data_split_text = text_splitter.split_documents(data)\n",
        "# text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=30)\n",
        "# data_split_graph = text_splitter2.split_documents(data_graph)\n",
        "\n",
        "# # embedding\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
        "# db_faiss_text = FAISS.from_documents(data_split_text, embeddings)\n",
        "# db_faiss_graph = FAISS.from_documents(data_split_graph, embeddings)\n",
        "\n",
        "# from rank_bm25 import BM25Okapi\n",
        "\n",
        "# def make_tok(sent):\n",
        "#   return sent.split(\" \")\n",
        "\n",
        "# corpus=  [i.page_content for i in data_split_text]\n",
        "# tokenized_corpus = [make_tok(i.page_content) for i in data_split_text]\n",
        "# bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# corpus_graph=  [i.page_content for i in data_split_graph]\n",
        "# tokenized_corpus_graph = [make_tok(i.page_content) for i in data_split_graph]\n",
        "# bm25_graph = BM25Okapi(tokenized_corpus_graph)\n",
        "# # search similarity - text\n",
        "# def db_search(query: str, k: int):\n",
        "#     docs = db_faiss_text.similarity_search(query, k)\n",
        "#     return docs\n",
        "\n",
        "# # search similarity - graph\n",
        "# def db_search_graph(query: str, k: int):\n",
        "#     docs = db_faiss_graph.similarity_search(query, k)\n",
        "#     return docs\n",
        "\n",
        "# # Bm25리트리버 - 4개씩 가져오도록 설정, 변경가능\n",
        "# bm25_retriever = BM25Retriever.from_texts(corpus)\n",
        "# bm25_retriever.k = 6\n",
        "# bm25_retriever_graph = BM25Retriever.from_texts(corpus_graph)\n",
        "# bm25_retriever_graph.k = 6\n",
        "\n",
        "# # faiss_retreiver\n",
        "# faiss_retriever = db_faiss_text.as_retriever(search_kwargs={\"k\": 10})\n",
        "# faiss_retriever_graph = db_faiss_graph.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# # 앙상블 가중치 - Bm25 가중치 0.2, faiss 0.8 가중치\n",
        "# ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
        "#                                        weights=[0.2, 0.8])\n",
        "\n",
        "# # 로컬 llm가져오기\n",
        "# def load_local_llm():\n",
        "#     model_id='mncai/llama2-13b-dpo-v3'\n",
        "#     bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
        "#     )\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#     model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
        "#     return model\n",
        "\n",
        "# model=load_local_llm()\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained('mncai/llama2-13b-dpo-v3')\n",
        "# streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# stop_list = ['### ','###','### example:']\n",
        "\n",
        "# stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "# stop_token_ids\n",
        "# stop_token_ids = [torch.LongTensor(x).to(0) for x in stop_token_ids]\n",
        "# stop_token_ids\n",
        "\n",
        "\n",
        "# # define custom stopping criteria object\n",
        "# class StopOnTokens(StoppingCriteria):\n",
        "#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "#         for stop_ids in stop_token_ids:\n",
        "#             if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "#                 return True\n",
        "#         return False\n",
        "\n",
        "# stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "\n",
        "# from konlpy.tag import Okt\n",
        "\n",
        "# #객체 생성\n",
        "\n",
        "# # 파일리스트는 가져옴\n",
        "# file_list=os.listdir('drive/MyDrive/[FINAL] 그래프 png 파일')\n",
        "# file_names = file_list\n",
        "\n",
        "# text=file_list\n",
        "# texts=[]\n",
        "# text2=[]\n",
        "# text3=[]\n",
        "# for i in text:\n",
        "#     i=unicodedata.normalize('NFC',i)\n",
        "#     i=i.replace('.png의','')\n",
        "#     #i=i.replace('_',' ')\n",
        "#     i=i.replace('.PNG의','')\n",
        "#     i=i.replace(' 사본','')\n",
        "\n",
        "#     text2.append(i) #okt.morphs(unicodedata.normalize('NFC',i)))#+okt.verbs(i))\n",
        "\n",
        "# list_of_documents = [\n",
        "#     Document(page_content=name, metadata=dict(mainsource=name.split('_')[0]))\n",
        "#     for name in text2\n",
        "# ]\n",
        "\n",
        "# db_faiss_graghs = FAISS.from_documents(list_of_documents, embeddings)\n",
        "\n",
        "#def 그래프 -> 한번 시도해보기. 질문과 가장 관련된 파일을 가져오기\n",
        "def graphs(x,docs_input):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=0.8,\n",
        "        top_k=100,\n",
        "        max_new_tokens=300,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    template = f\"\"\"### instruction:\n",
        "        아래 문서는 그래프에 대한 해석이다.\n",
        "        문서에 주어진 내용만을 이용해서 답하고 문서에서 근거를 찾을 수 없거나 답변하기 모호하면 정보를 찾을 수 없습니다. 라고 답변해줘.\n",
        "\n",
        "        문서: {docs_input}\n",
        "        질문 = {x}\\n\\n### Response:\n",
        "        output ():\n",
        "    \"\"\"\n",
        "\n",
        "    q = template\n",
        "\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            q,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ).to('cuda'),\n",
        "        generation_config=generation_config,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    result_str = tokenizer.decode(gened[0])\n",
        "\n",
        "    start_tag = f\"[/INST]\"\n",
        "    start_index = result_str.find(start_tag)\n",
        "\n",
        "    if start_index != -1:\n",
        "        result_str = result_str[start_index + len(start_tag):].strip()\n",
        "    return result_str\n",
        "\n",
        "# 질문과 관련된 비슷한 질문 생성\n",
        "def make_similar_query(x):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=0.8,\n",
        "        top_k=100,\n",
        "        max_new_tokens=60,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    # 비슷한 문장을 두개 만들어줌. 쿼리가 조금만 달라져도 가지고 오는 문서가 다르기에 - 질문과 관련된 문서 가지고올 확률 높아짐\n",
        "    template = f\"\"\"### instruction:\n",
        "        질문과 관련된 2개의 다중 검색 질문를 생성해줘.\n",
        "\n",
        "        질문= {x} ### Response:\n",
        "        output (주어진 질문과 유사한 질문 2개):\n",
        "    \"\"\"\n",
        "\n",
        "    q = template\n",
        "\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            q,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ).to('cuda'),\n",
        "        generation_config=generation_config,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    result_str = tokenizer.decode(gened[0])\n",
        "\n",
        "    start_tag = f\"[/INST]\"\n",
        "    start_index = result_str.find(start_tag)\n",
        "\n",
        "    if start_index != -1:\n",
        "        result_str = result_str[start_index + len(start_tag):].strip()\n",
        "    return result_str\n",
        "\n",
        "\n",
        "# find_core 쿼리에 관련된 답변을 1차적으로 생성해 리트리버의 input으로 넣어줌\n",
        "# 하이드 방법론 - 1차적으로 답변한 것으로 리트리버가 문서 갖고 오도록 유사어와 동의어 들어왔을 때 llm이 인지해서 답변 리트리버가 명확한 답을 가져올 확률이 높다\n",
        "def find_core(x):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        top_k=100,\n",
        "        max_new_tokens=40,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    template = f\"\"\"### instruction:\n",
        "        사실만을 말하는 금융 전문가로서 답해줘.\n",
        "\n",
        "        질문: {x}\\n\\n### Response:\n",
        "\n",
        "    \"\"\"\n",
        "    q = template\n",
        "\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            q,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ).to('cuda'),\n",
        "        generation_config=generation_config,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    result_str = tokenizer.decode(gened[0])\n",
        "\n",
        "    start_tag = f\"[/INST]\"\n",
        "    start_index = result_str.find(start_tag)\n",
        "\n",
        "    if start_index != -1:\n",
        "        result_str = result_str[start_index + len(start_tag):].strip()\n",
        "    return result_str\n",
        "\n",
        "\n",
        "# 최종 답변을 생성하는 함수\n",
        "def gen_final(x,docs_input):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=0.8,\n",
        "        top_k=100,\n",
        "        max_new_tokens=300,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "    # 많이 시도하면서 보완하면서 작성. 답변을 잘 하지 못하는 케이스 파악 후 보완\n",
        "    template = f\"\"\"### instruction:\n",
        "        너는 사실만을 말하는 금융 전문가야.\n",
        "        질문이 주어진 문서의 내용과 관련이 있다면, 해당 문서를 바탕으로 구체적이고 명확한 답변을 해줘.\n",
        "        모든 문서의 내용을 이해하고 고려해서 질문과 관련된 정보를 모두 찾아 답변 해줘.\n",
        "        만약 질문에 '모두', '전부'와 같은 단어가 포함되어 있다면 해당하는 정보를 문서에서 모두 최대한 찾아서 답변해줘.\n",
        "        문서에 존재하지 않는 내용은 답하면 안돼.\n",
        "        질문에 연도가 포함되어 있다면, 그 연도에 해당하는 정확한 데이터를 제공해줘.\n",
        "        만약 질문에 연도가 명시되지 않았다면, 현재 연도인 2023년의 정보를 사용해줘.\n",
        "        국내 시장과 관련된 질문이면 '내수'로 이해하고 답변해줘.\n",
        "        비율이나 성장률, 이익률 등에 대한 질문에는 그에 해당하는 비율 값을 제공해줘.\n",
        "        대답은 완성된 문장으로 해주고 완성하지 못할것 같으면 이전 문장까지만 만들어줘.\n",
        "        문서 내용과 관련 없거나 애매한 질문이면 '내용을 찾을 수 없습니다.'라고 답변해줘.\n",
        "        소속이나 존재를 묻는 질문은 문서에 정확하게 일치하는 내용이 없다면 없다고 답변해줘\n",
        "        필요한 답변만 하고 불필요한 답변은 생성하면 안돼.\n",
        "        문서에서 정보를 얻을 수 없다면 임의로 생성하지 말고 반드시 '문서에서 내용을 찾을 수 없습니다.'라고 답변해줘.\n",
        "        문서들: {docs_input}\n",
        "        질문: {x}\\n\\n### Response:\n",
        "    \"\"\"\n",
        "\n",
        "    q = template\n",
        "\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            q,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ).to('cuda'),\n",
        "        generation_config=generation_config,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        stopping_criteria=stopping_criteria,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    result_str = tokenizer.decode(gened[0])\n",
        "\n",
        "    start_tag = f\"[/INST]\"\n",
        "    start_index = result_str.find(start_tag)\n",
        "\n",
        "    if start_index != -1:\n",
        "        result_str = result_str[start_index + len(start_tag):].strip()\n",
        "    return result_str\n",
        "\n",
        "def filter_documents_by_query(documents, query):\n",
        "    # 쿼리에서 키워드 확인\n",
        "    query_keywords = [unicodedata.normalize('NFD', keyword) for keyword in [\"하이브\", \"퀄컴\", \"자동차\", \"롯데\",\"현대\",\"기아\",\"미국\"] if keyword in query]\n",
        "    query_keywords2 = [\"하이브\", \"퀄컴\", \"자동차\", \"롯데\",\"현대\",\"기아\",\"미국\"]\n",
        "\n",
        "    # 먼저 pdf의 종류를 고르고, 아래에 해당되는 키워드가 있는지 살펴서 그래프를 가져옴\n",
        "    # query_keywords2 = [unicodedata.normalize('NFD', keyword) for keyword in ['매출','재고','영업이익','EPS','eps'] if keyword in query]\n",
        "    # 해당 키워드를 mainsource로 가지는 문서 필터링\n",
        "    if len(query_keywords)>0:\n",
        "      filtered_docs = [doc for doc in documents if (any(item in doc.metadata.get('source', '') for item in query_keywords)) or (any(item in doc.page_content for item in query_keywords + query_keywords2))]\n",
        "    else:\n",
        "      filtered_docs= [doc for doc in documents]\n",
        "    return filtered_docs\n",
        "\n",
        "from langchain.document_transformers import (\n",
        "    LongContextReorder,\n",
        ")\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "# 딕셔너리 형태\n",
        "keyword_replacements = {\n",
        "    'hybe': '하이브',\n",
        "    'lotte': '롯데',\n",
        "    'hyundai': '현대',\n",
        "    'kia': '기아',\n",
        "    'qualcomm': '퀄컴',\n",
        "    'bts' : 'BTS'\n",
        "}\n",
        "\n",
        "def replace_keywords(query):\n",
        "    for eng, kor in keyword_replacements.items():\n",
        "        # re.IGNORECASE를 사용하여 대소문자 구분 없이 검색 및 대체\n",
        "        query = re.sub(re.compile(eng, re.IGNORECASE), kor, query)\n",
        "    return query\n",
        "\n",
        "\n",
        "query=''\n",
        "'''여기서 부터 쿼리 받아서 질답!'''\n",
        "while(query!='exit'):\n",
        "    query=input(\"\\n원하는 질문을 입력하세요. 종료를 원한다면 'exit'라고 입력하세요 : \\n\\n\")\n",
        "    query = replace_keywords(query)\n",
        "\n",
        "    # 쿼리에 그래프가 언급되어 있으면 그래프 이미지를 제시하면서 답변한다\n",
        "    if '그래프' in query:\n",
        "      results_with_scores = db_faiss_graghs.similarity_search(query,6)\n",
        "      graph_docs=[docssss.page_content for docssss in results_with_scores]\n",
        "\n",
        "      for graph_index,gg in enumerate(graph_docs[0:3]):\n",
        "        print(str(graph_index+1)+'. '+str(gg))\n",
        "\n",
        "      select_graph=input('\\n찾으시는 그래프의 번호를 입력 해주세요. 만약 존재하지 않는다면 N을 입력 해주세요. ex) 1번 \\n\\n')\n",
        "\n",
        "      if select_graph=='N' or select_graph=='n':\n",
        "        for graph_index2,ggg in enumerate(graph_docs[3:6]):\n",
        "          print(graph_index2+4,'.',ggg)\n",
        "\n",
        "        select_graph2=input('\\n다시 한번 찾으시는 그래프의 번호를 입력 해주세요. 만약 존재하지 않는다면 N을 입력 해주세요. ex) 1번 \\n\\n')\n",
        "\n",
        "        if select_graph2=='N' or select_graph2=='n':\n",
        "          print('\\n죄송합니다. 해당 문서에는 관련한 그래프가 존재하지 않습니다.\\n')\n",
        "        else:\n",
        "          ss=graph_docs[int(select_graph2[0])-1]\n",
        "          try:\n",
        "            img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+ss+'.png의 사본')\n",
        "            plt.imshow(img_test)\n",
        "            plt.show()\n",
        "\n",
        "          except:\n",
        "            img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+ss+'.PNG의 사본')\n",
        "            plt.imshow(img_test)\n",
        "            plt.show()\n",
        "          f = open('/content/drive/MyDrive/[FINAL] 그래프 전처리/'+ss.split('_')[0]+'/'+ss+'.txt','r', encoding='utf-8')     # mode = 부분은 생략해도 됨\n",
        "          lines = f.readlines()\n",
        "\n",
        "          # 각 줄을 '.' 기준으로 분리하여 출력\n",
        "          for line in lines:\n",
        "              parts = line.split('.')\n",
        "              for part in parts:\n",
        "                  print(part)\n",
        "      else:\n",
        "        sss=graph_docs[int(select_graph[0])-1]\n",
        "        try:\n",
        "          img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+sss+'.png의 사본')\n",
        "          plt.imshow(img_test)\n",
        "          plt.show()\n",
        "        except:\n",
        "          img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+sss+'.PNG의 사본')\n",
        "          plt.imshow(img_test)\n",
        "          plt.show()\n",
        "        f = open('/content/drive/MyDrive/[FINAL] 그래프 전처리/'+sss.split('_')[0]+'/'+sss+'.txt','r', encoding='utf-8')     # mode = 부분은 생략해도 됨\n",
        "        lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "          parts = line.split('.')\n",
        "          for part in parts:\n",
        "            part.replace('\\n\\n','')\n",
        "            print(part)\n",
        "\n",
        "\n",
        "      if (query=='exit'):\n",
        "        print('\\n종료합니다.')\n",
        "        break\n",
        "\n",
        "    else:\n",
        "      if (query=='exit'):\n",
        "        print('\\n종료합니다.')\n",
        "        break\n",
        "      outputs=make_similar_query(query)\n",
        "\n",
        "      # 1차적인 답변 생성\n",
        "      core=find_core(query)\n",
        "      docs = ensemble_retriever.get_relevant_documents(query)\n",
        "      #docs = [doc.page_content for doc in docs]\n",
        "      #core = ','.join(okt.nouns(query))\n",
        "      docs2 =ensemble_retriever.get_relevant_documents(core)\n",
        "      #docs2 = [doc.page_content for doc in docs2]\n",
        "      docs3 =ensemble_retriever.get_relevant_documents(outputs.split('\\n')[1])\n",
        "      #docs3 = [doc.page_content for doc in docs3]\n",
        "      docs4 =ensemble_retriever.get_relevant_documents(outputs.split('\\n')[2].replace('</s>',''))\n",
        "      #docs4 = [doc.page_content for doc in docs4]\n",
        "      docs_all=(docs+docs2+docs3+docs4)\n",
        "      documents=docs_all\n",
        "\n",
        "      # 예시 사용\n",
        "      filtered_documents = filter_documents_by_query(documents, query)\n",
        "\n",
        "      # 필터링된 문서 정보\n",
        "      # filtered_documents_info = [(doc.page_content, doc.metadata) for doc in filtered_documents]\n",
        "      reordering = LongContextReorder()\n",
        "      reordered_docs = reordering.transform_documents(filtered_documents)\n",
        "      reordered_docs = [doc.page_content for doc in reordered_docs]\n",
        "      reordered_docs=set(reordered_docs)\n",
        "\n",
        "      #Reorded_docs가 최종 쿼리가 된다\n",
        "      query_final=reordered_docs\n",
        "\n",
        "      print('\\n')\n",
        "      gen_final(query,query_final)\n",
        "#       # print('답변 : ',gen_final(query,query_final))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "[ ]\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsEpeuPb74mJ",
        "outputId": "521ff358-da22-4e40-fdb1-69553aa20715"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "from tkinter import scrolledtext\n",
        "\n",
        "# 사용자의 질문을 처리하는 함수\n",
        "def handle_query():\n",
        "    query = query_entry.get()\n",
        "    # 'exit' 입력 시 프로그램 종료\n",
        "    if query.lower() == 'exit':\n",
        "        window.destroy()\n",
        "        return\n",
        "    # 여기에 query 처리 로직을 추가하세요. 예: gen_final(query)\n",
        "    # 결과를 result_area에 출력\n",
        "    while(query!='exit'):\n",
        "      query=input(\"\\n원하는 질문을 입력하세요. 종료를 원한다면 'exit'라고 입력하세요 : \\n\\n\")\n",
        "      query = replace_keywords(query)\n",
        "\n",
        "     # 쿼리에 그래프가 언급되어 있으면 그래프 이미지를 제시하면서 답변한다\n",
        "      if '그래프' in query:\n",
        "        results_with_scores = db_faiss_graghs.similarity_search(query,6)\n",
        "        graph_docs=[docssss.page_content for docssss in results_with_scores]\n",
        "\n",
        "        for graph_index,gg in enumerate(graph_docs[0:3]):\n",
        "          print(str(graph_index+1)+'. '+str(gg))\n",
        "\n",
        "        select_graph=input('\\n찾으시는 그래프의 번호를 입력 해주세요. 만약 존재하지 않는다면 N을 입력 해주세요. ex) 1번 \\n\\n')\n",
        "\n",
        "        if select_graph=='N' or select_graph=='n':\n",
        "          for graph_index2,ggg in enumerate(graph_docs[3:6]):\n",
        "            print(graph_index2+4,'.',ggg)\n",
        "\n",
        "          select_graph2=input('\\n다시 한번 찾으시는 그래프의 번호를 입력 해주세요. 만약 존재하지 않는다면 N을 입력 해주세요. ex) 1번 \\n\\n')\n",
        "\n",
        "          if select_graph2=='N' or select_graph2=='n':\n",
        "            print('\\n죄송합니다. 해당 문서에는 관련한 그래프가 존재하지 않습니다.\\n')\n",
        "          else:\n",
        "            ss=graph_docs[int(select_graph2[0])-1]\n",
        "            try:\n",
        "              img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+ss+'.png의 사본')\n",
        "              plt.imshow(img_test)\n",
        "              plt.show()\n",
        "\n",
        "            except:\n",
        "              img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+ss+'.PNG의 사본')\n",
        "              plt.imshow(img_test)\n",
        "              plt.show()\n",
        "            f = open('/content/drive/MyDrive/[FINAL] 그래프 전처리/'+ss.split('_')[0]+'/'+ss+'.txt','r', encoding='utf-8')     # mode = 부분은 생략해도 됨\n",
        "            lines = f.readlines()\n",
        "\n",
        "            # 각 줄을 '.' 기준으로 분리하여 출력\n",
        "            for line in lines:\n",
        "                parts = line.split('.')\n",
        "                for part in parts:\n",
        "                    print(part)\n",
        "        else:\n",
        "          sss=graph_docs[int(select_graph[0])-1]\n",
        "          try:\n",
        "            img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+sss+'.png의 사본')\n",
        "            plt.imshow(img_test)\n",
        "            plt.show()\n",
        "          except:\n",
        "            img_test = img.imread('drive/MyDrive/[FINAL] 그래프 png 파일/'+sss+'.PNG의 사본')\n",
        "            plt.imshow(img_test)\n",
        "            plt.show()\n",
        "          f = open('/content/drive/MyDrive/[FINAL] 그래프 전처리/'+sss.split('_')[0]+'/'+sss+'.txt','r', encoding='utf-8')     # mode = 부분은 생략해도 됨\n",
        "          lines = f.readlines()\n",
        "\n",
        "          for line in lines:\n",
        "            parts = line.split('.')\n",
        "            for part in parts:\n",
        "              part.replace('\\n\\n','')\n",
        "              print(part)\n",
        "\n",
        "\n",
        "        if (query=='exit'):\n",
        "          print('\\n종료합니다.')\n",
        "          break\n",
        "\n",
        "      else:\n",
        "        if (query=='exit'):\n",
        "          print('\\n종료합니다.')\n",
        "          break\n",
        "        outputs=make_similar_query(query)\n",
        "\n",
        "        # 1차적인 답변 생성\n",
        "        core=find_core(query)\n",
        "        docs = ensemble_retriever.get_relevant_documents(query)\n",
        "        #docs = [doc.page_content for doc in docs]\n",
        "        #core = ','.join(okt.nouns(query))\n",
        "        docs2 =ensemble_retriever.get_relevant_documents(core)\n",
        "        #docs2 = [doc.page_content for doc in docs2]\n",
        "        docs3 =ensemble_retriever.get_relevant_documents(outputs.split('\\n')[1])\n",
        "        #docs3 = [doc.page_content for doc in docs3]\n",
        "        docs4 =ensemble_retriever.get_relevant_documents(outputs.split('\\n')[2].replace('</s>',''))\n",
        "        #docs4 = [doc.page_content for doc in docs4]\n",
        "        docs_all=(docs+docs2+docs3+docs4)\n",
        "        documents=docs_all\n",
        "\n",
        "        # 예시 사용\n",
        "        filtered_documents = filter_documents_by_query(documents, query)\n",
        "\n",
        "        # 필터링된 문서 정보\n",
        "        # filtered_documents_info = [(doc.page_content, doc.metadata) for doc in filtered_documents]\n",
        "        reordering = LongContextReorder()\n",
        "        reordered_docs = reordering.transform_documents(filtered_documents)\n",
        "        reordered_docs = [doc.page_content for doc in reordered_docs]\n",
        "        reordered_docs=set(reordered_docs)\n",
        "\n",
        "        #Reorded_docs가 최종 쿼리가 된다\n",
        "        query_final=reordered_docs\n",
        "\n",
        "        print('\\n')\n",
        "        gen_final(query,query_final)\n",
        "    result_area.insert(tk.INSERT, \"여기에 결과를 출력합니다.\\n\")\n",
        "\n",
        "# GUI 창 초기화\n",
        "window = tk.Tk()\n",
        "window.title(\"질문 입력 인터페이스\")\n",
        "\n",
        "# 질문 입력 필드\n",
        "query_label = tk.Label(window, text=\"질문을 입력하세요:\")\n",
        "query_label.pack()\n",
        "query_entry = tk.Entry(window, width=50)\n",
        "query_entry.pack()\n",
        "\n",
        "# '질문하기' 버튼\n",
        "query_button = tk.Button(window, text=\"질문하기\", command=handle_query)\n",
        "query_button.pack()\n",
        "\n",
        "# 결과 출력 영역\n",
        "result_area = scrolledtext.ScrolledText(window, width=60, height=10)\n",
        "result_area.pack()\n",
        "\n",
        "# GUI 실행\n",
        "window.mainloop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "gMpyr6N0QGI-",
        "outputId": "5563f7a0-a062-46be-f559-342cc0dedd56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d893e0b13a79>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# GUI 창 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"질문 입력 인터페이스\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2297\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}